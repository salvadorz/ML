{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes\n",
    "Naive Bayes is a popular supervised learning method used for classification tasks. It is based on the application of [Baye's Theorem](https://salvadorz.atlassian.net/wiki/spaces/SYS/pages/33882128/Bayes+Theorem \"Baye's Theorem\") with the assumption of feature independence, known as the \"naive\" assumption. Despite its simplicity, Naive Bayes can achieve good performance and is particularly effective in text classification and spam filtering tasks.\n",
    "\n",
    "The concept behind Naive Bayes is to calculate the probability of a given sample belonging to each class and then classify the sample based on the highest probability. It utilizes Bayes' theorem, which calculates the posterior probability of a class given the observed features.\n",
    "\n",
    "Mathematically, the Naive Bayes classifier can be represented as:\n",
    "$$ \\large P(class|features)= \\frac{P(features|class)}{P(features)}P(class) $$\n",
    "\n",
    "To estimate these probabilities, different approaches are used based on the type of data and the distribution assumptions. Some common approaches include:\n",
    "\n",
    "* **Multinomial Naive Bayes**:\n",
    "    This approach is suitable for discrete features, often used in text classification. It assumes that the features follow a multinomial distribution, and the probabilities are calculated based on the frequency of each feature.\n",
    "\n",
    "* **Gaussian Naive Bayes**:\n",
    "    This approach assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous or real-valued features. The probabilities are calculated based on the mean and variance of each feature in each class.\n",
    "\n",
    "* **Bernoulli Naive Bayes**:\n",
    "    This approach is similar to Multinomial Naive Bayes but assumes that the features are binary variables. It is commonly used for document classification tasks, where the presence or absence of specific words determines the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 0 is not spam\n",
      "Email 1 is not spam\n",
      "Email 2 is not spam\n",
      "Email 3 is spam\n",
      "Email 4 is spam\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes, Bernoulli version. Bernoulli uses binary feature vectors,\n",
    "# see: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "# from CS 229 Standford University.\n",
    "# Classifier : Spam filter example\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "# Each feature vector's length is equal to the number of words in \"the dictionary\".\n",
    "# The dictionary is incredibly short, just 16 words\n",
    "# The depth of the array is equal to the number m of training emails. There are examples of\n",
    "# 8 non-spam emails, and 8 spam emails.\n",
    "# Dictionary words:\n",
    "# hello goodbye dave bill ted rufus vacation dinner restaurant eating drinking sleeping equal the price buy\n",
    "#   0     1       2   3    4   5        6      7        8        9      10       11      12    13  14   15\n",
    "\n",
    "# These are word occurance vectors represented by 1=present, 0=absent in the positions shown above\n",
    "#   ===>>> where we translate features into values: present or absent\n",
    "X = np.array(\n",
    "    [\n",
    "    # non-spam emails\n",
    "    # These are our P(x|y)'s = 0, NOT SPAM\n",
    "    [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0], # 0, hello dave ted rufus vacation restaurant drinking equal the\n",
    "    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0], # 1\n",
    "    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0], # 2\n",
    "    [0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0], # 3\n",
    "    [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0], # 4\n",
    "    [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0], # 5\n",
    "    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0], # 6\n",
    "    [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0], # 7\n",
    "    # spam emails, contains the words price, buy or price+buy\n",
    "    # These are our P(x|y)'s = 1, SPAM\n",
    "    [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0], # 8\n",
    "    [0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1], # 9\n",
    "    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1], # 10\n",
    "    [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1], # 11\n",
    "    [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0], # 12\n",
    "    [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1], # 13\n",
    "    [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1], # 14\n",
    "    [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1]  # 15\n",
    "    ]\n",
    ") \n",
    "\n",
    "# Outcomes (the y's). First 8 are not spam, second 8 are spam\n",
    "#   ===>>> where we translate outcomes into values: not-spam or spam\n",
    "Y = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes uses X and Y to calculate P(y|x)\n",
    "#    What is the probability that email \"x\" is or is not spam?\n",
    "# test feature vectors representing new emails\n",
    "TEST_X = np.array(\n",
    "    [\n",
    "    # 5 new emails\n",
    "    [0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], # 0\n",
    "    [1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0], # 1\n",
    "    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0], # 2 price\n",
    "    [1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1], # 3 buy\n",
    "    [1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1]  # 4 price+buy\n",
    "    ]\n",
    ") \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_nb = BernoulliNB()\n",
    "\n",
    "# Train\n",
    "bernoulli_nb.fit(X, Y)\n",
    "\n",
    "\n",
    "# Set params\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "\n",
    "m, n = TEST_X.shape\n",
    "for index in range(m) :\n",
    "    \n",
    "    test = TEST_X[index]      # extract a row, test becomes a column vector\n",
    "    test = test.reshape(1,n)  # reshape it into a row vector, what the .predict() method expects\n",
    "    \n",
    "    # Predict\n",
    "    y_prediction = bernoulli_nb.predict(test)\n",
    "    if (y_prediction[0] == 0) :\n",
    "        print (\"Email\", index, \"is not spam\")\n",
    "    else :\n",
    "        print (\"Email\", index, \"is spam\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this Example we can see failed only on `1` case but this can be adjusted with the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
